{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import pickle\n",
    "import re\n",
    "from functools import reduce\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/trump_source_sentiment_df.pkl', 'rb') as handle:\n",
    "    trump_source = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/trump_shares_sentiment_df.pkl', 'rb') as handle:\n",
    "    trump_shares = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/clinton_source_sentiment_df.pkl', 'rb') as handle:\n",
    "    clinton_source = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/clinton_shares_sentiment_df.pkl', 'rb') as handle:\n",
    "    clinton_shares = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all dataframes - 'origin' column identifies the original source\n",
    "\n",
    "frames = [trump_source, trump_shares, clinton_source, clinton_shares]\n",
    "df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpacking Nested Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to unpack nested features from columns containing dictionaries\n",
    "\n",
    "def unpack_column(df, input_column, key, output_column):\n",
    "    try:\n",
    "        df[output_column] = df[input_column].apply(lambda x: x.get(key))\n",
    "    except:\n",
    "        df[output_column] = np.NaN\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = unpack_column(df, 'source', 'uri', 'news_source')\n",
    "df = unpack_column(df, 'shares', 'facebook', 'facebook_shares')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Satirical News Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "satire = ['theonion.com', 'cracked.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['news_source'].isin(satire)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load custom stop words to be used along with NLTK stop words\n",
    "\n",
    "article_stop_words = pd.read_csv('../settings/stop_words.csv', header=None)\n",
    "trump_stop_words = pd.read_csv('../settings/trump_title_stop_words.csv', header=None)\n",
    "clinton_stop_words = pd.read_csv('../settings/clinton_title_stop_words.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_stop_words = set(article_stop_words.iloc[:,0])\n",
    "trump_stop_words = set(trump_stop_words.iloc[:,0])\n",
    "clinton_stop_words = set(clinton_stop_words.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dateTime column to be used with Grouper\n",
    "\n",
    "df['date_format'] = pd.to_datetime(df['dateTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create nested dictionary containing word frequencies\n",
    "\n",
    "def word_frequency(df, origin_column, origin_list, resample_list, dateTime_column, text_column, num_words, stop_words):\n",
    "    freq_dict = {}\n",
    "    tokenizer = tokenize.RegexpTokenizer(r'\\w+')\n",
    "    for origin in origin_list:\n",
    "        freq_dict[origin] = {}\n",
    "        temp_df = df.loc[df[origin_column] == origin]\n",
    "        for resample in resample_list:\n",
    "            freq_dict[origin][resample]={}\n",
    "            temp_group = temp_df.set_index(dateTime_column).resample(resample).agg({text_column: ' '.join})\n",
    "            for index, row in temp_group.iterrows():\n",
    "                freq_dict[origin][resample][index.strftime('%m/%d/%Y')] = {}\n",
    "                words = tokenizer.tokenize(row[text_column])\n",
    "                words = [word for word in words if not word.isnumeric()]\n",
    "                words = [word.lower() for word in words]\n",
    "                words = [word for word in words if word not in stop_words]\n",
    "                fdist = FreqDist(words)\n",
    "                for word, frequency in fdist.most_common(num_words):\n",
    "                    freq_dict[origin][resample][index.strftime('%m/%d/%Y')][word] = frequency\n",
    "    return freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create necessary lists and sets to be used in the word_frequency (and later) function(s)\n",
    "\n",
    "article_origins = ['trump_source', 'trump_shares', 'clinton_source', 'clinton_shares']\n",
    "trump_origins = ['trump_source', 'trump_shares']\n",
    "clinton_origins = ['clinton_source', 'clinton_shares']\n",
    "resamples = ['W-SAT', 'M']\n",
    "article_stop_words = article_stop_words.union(set(stopwords.words('english')))\n",
    "trump_stop_words = trump_stop_words.union(set(stopwords.words('english')))\n",
    "clinton_stop_words = clinton_stop_words.union(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create article word frequency nested dictionary\n",
    "\n",
    "article_words = word_frequency(df, 'origin', trump_origins, resamples, 'date_format', 'body', 20, trump_stop_words)\n",
    "clinton_article_words = word_frequency(df, 'origin', clinton_origins, resamples, 'date_format', 'body', 20, clinton_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the dictionaries\n",
    "\n",
    "article_words.update(clinton_article_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create title word frequency nested dictionary\n",
    "\n",
    "title_words = word_frequency(df, 'origin', trump_origins, resamples, 'date_format', 'title', 20, trump_stop_words)\n",
    "clinton_title_words = word_frequency(df, 'origin', clinton_origins, resamples, 'date_format', 'title', 20, clinton_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the dictionaries\n",
    "\n",
    "title_words.update(clinton_title_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create nested dictionary containing mean sentiment scores for each origin and time period (week/month)\n",
    "\n",
    "def sent_summary(df, origin_column, origin_list, resample_list, dateTime_column, sentiment_column):\n",
    "    sentiment_dict = {}\n",
    "    for origin in origin_list:\n",
    "        sentiment_dict[origin] = {}\n",
    "        temp_df = df.loc[df[origin_column] == origin]\n",
    "        for resample in resample_list:\n",
    "            sentiment_dict[origin][resample]={}\n",
    "            temp_group = temp_df.set_index(dateTime_column).resample(resample).agg({sentiment_column: 'mean'})\n",
    "            for index, row in temp_group.iterrows():\n",
    "                sentiment_dict[origin][resample][index.strftime('%m/%d/%Y')] = row[sentiment_column]\n",
    "    return sentiment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create article mean sentiment nested dictionary\n",
    "\n",
    "article_sentiment = sent_summary(df, 'origin', article_origins, resamples, 'date_format', 'articleSentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create title mean sentiment nested dictionary\n",
    "\n",
    "title_sentiment = sent_summary(df, 'origin', article_origins, resamples, 'date_format', 'titleSentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create nested dictionary containing counts for the 20 news sources that produced the most articles in each group \n",
    "\n",
    "def source_counts(df, origin_column, origin_list, resample_list, dateTime_column, source_column, count_column):\n",
    "    source_dict = {}\n",
    "    for origin in origin_list:\n",
    "        source_dict[origin] = {}\n",
    "        temp_df = df.loc[df[origin_column] == origin]\n",
    "        for resample in resample_list:\n",
    "            source_dict[origin][resample]={}\n",
    "            temp_group = temp_df[[dateTime_column, source_column, count_column]].set_index(dateTime_column).groupby(pd.Grouper(freq = resample))\n",
    "            temp_series = temp_group.apply(lambda x: x.groupby(source_column).count().sort_values([count_column], ascending=False).head(20).apply(list).to_dict())\n",
    "            for index, item in temp_series.iteritems():\n",
    "                source_dict[origin][resample][index.strftime('%m/%d/%Y')]={}\n",
    "                for k1, v1 in item.items():\n",
    "                    for k2, v2 in v1.items():\n",
    "                        source_dict[origin][resample][index.strftime('%m/%d/%Y')][k2] = v2\n",
    "    return source_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create source count nested dictionary\n",
    "\n",
    "sources = source_counts(df, 'origin', article_origins, resamples, 'date_format', 'news_source', 'title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Share Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create nested dictionary containing mean shares for the 20 news sources that produced the most mean shares in each group \n",
    "\n",
    "def mean_share_counts(df, origin_column, origin_list, resample_list, dateTime_column, source_column, count_column):\n",
    "    shares_dict = {}\n",
    "    for origin in origin_list:\n",
    "        shares_dict[origin] = {}\n",
    "        temp_df = df.loc[df[origin_column] == origin]\n",
    "        for resample in resample_list:\n",
    "            shares_dict[origin][resample]={}\n",
    "            temp_group = temp_df[[dateTime_column, source_column, count_column]].set_index(dateTime_column).groupby(pd.Grouper(freq = resample))\n",
    "            temp_series = temp_group.apply(lambda x: x.groupby(source_column).mean().sort_values([count_column], ascending=False).head(20).apply(list).to_dict())\n",
    "            for index, item in temp_series.iteritems():\n",
    "                shares_dict[origin][resample][index.strftime('%m/%d/%Y')]={}\n",
    "                for k1, v1 in item.items():\n",
    "                    for k2, v2 in v1.items():\n",
    "                        shares_dict[origin][resample][index.strftime('%m/%d/%Y')][k2] = v2\n",
    "    return shares_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create share count nested dictionary\n",
    "\n",
    "mean_shares = mean_share_counts(df, 'origin', article_origins, resamples, 'date_format', 'news_source', 'facebook_shares')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Share Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create nested dictionary containing total shares for the 20 news sources that produced the most total shares in each group \n",
    "\n",
    "def total_share_counts(df, origin_column, origin_list, resample_list, dateTime_column, source_column, count_column):\n",
    "    shares_dict = {}\n",
    "    for origin in origin_list:\n",
    "        shares_dict[origin] = {}\n",
    "        temp_df = df.loc[df[origin_column] == origin]\n",
    "        for resample in resample_list:\n",
    "            shares_dict[origin][resample]={}\n",
    "            temp_group = temp_df[[dateTime_column, source_column, count_column]].set_index(dateTime_column).groupby(pd.Grouper(freq = resample))\n",
    "            temp_series = temp_group.apply(lambda x: x.groupby(source_column).sum().sort_values([count_column], ascending=False).head(20).apply(list).to_dict())\n",
    "            for index, item in temp_series.iteritems():\n",
    "                shares_dict[origin][resample][index.strftime('%m/%d/%Y')]={}\n",
    "                for k1, v1 in item.items():\n",
    "                    for k2, v2 in v1.items():\n",
    "                        shares_dict[origin][resample][index.strftime('%m/%d/%Y')][k2] = v2\n",
    "    return shares_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create share count nested dictionary\n",
    "\n",
    "total_shares = total_share_counts(df, 'origin', article_origins, resamples, 'date_format', 'news_source', 'facebook_shares')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Share Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date_format_date'] = df['date_format'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_histogram = df[['date_format_date', 'origin', 'title', 'news_source', 'facebook_shares', 'articleSentiment', 'titleSentiment']].copy().set_index('date_format_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_histogram.insert(0, 'row_id', range(0, len(df_histogram)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data for Use in App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/article_wordcount_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(article_words, f, pickle.HIGHEST_PROTOCOL)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/title_wordcount_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(title_words, f, pickle.HIGHEST_PROTOCOL)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/article_sentiment_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(article_sentiment, f, pickle.HIGHEST_PROTOCOL)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/title_sentiment_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(title_sentiment, f, pickle.HIGHEST_PROTOCOL)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/source_count_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(sources, f, pickle.HIGHEST_PROTOCOL)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/mean_share_count_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(mean_shares, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/total_share_count_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(total_shares, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/df_histogram.pkl', 'wb') as f:\n",
    "    pickle.dump(df_histogram, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
